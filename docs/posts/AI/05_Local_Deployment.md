# 05. 本地部署原理：NPU 崛起与端侧 AI (2026 版)

> [!NOTE]
> **硬件革命**
> 
> 在 2024 年，大家还在讨论买 4090 显卡。
> 到了 2026 年，**NPU (Neural Processing Unit)** 已经成为 PC 和手机的标配。AI 推理正在从“云端 GPU”迁移到“本地 NPU”。

## 1. 为什么 GPU 不是最优解？

GPU (Graphics Processing Unit) 是为了画图（渲染像素）设计的，它的特点是 SIMD（单指令多数据）。
但 LLM 推理的特点是：
*   **Memory Bound (存内计算)**: 瓶颈不在计算，而在每秒能搬运多少数据 (Bandwidth)。
*   **Sequential (时序性)**: 一个 Token 接一个 Token，GPU 的几千个核心大部分时间在空转等待。

## 2. 2026 新物种：LPU与专有 NPU

### LPU (Language Processing Unit)
Groq 等公司推出的 LPU 架构，抛弃了 HBM (高带宽显存)，直接使用超高速 SRAM。
*   **速度**: > 500 Token/s (人类阅读速度的 10 倍)。
*   **原理**: 确定性数据流。像流水线一样安排好每个时钟周期的计算，没有 Cache Miss。

### 手机里的 NPU
苹果 A 系列、高通骁龙、华为麒麟的 NPU 算力已突破 100 TOPS (INT8)。
这意味着：**7B 甚至 13B 模型可以在手机上流畅运行，不发热，不耗电。**

## 3. 量化技术 (Quantization) ... (保留 INT4/GGUF 内容)
## 4. 推理文件格式 (GGUF) ... (保留内容)

## 小结

2026 年的本地部署发生了质变：
1.  **端云协同**: 简单的任务（回消息、日程）由手机 NPU 处理，复杂的任务（写代码、科研）通过云端 Hybrid 模型处理。
2.  **隐私第一**: 个人数据不需要上传云端，本地模型就是私人助理。
